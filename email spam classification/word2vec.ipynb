{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec Implementation**\n",
    "----------------------------------\n",
    "__________________________________\n",
    "\n",
    "1. [Data Preprocessing](#Data-Preprocessing)\n",
    "2. [Word2Vec Implementation](#Word2Vec-Implementation)\n",
    "3. [Data Augmentation (Optional)](#Data-Augmentation-(Optional))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Convert all text to Lowercase\n",
    "def lowercase(corpus):\n",
    "    return corpus.str.lower()\n",
    "\n",
    "# Remove Punctuation from the corpus\n",
    "punct_and_nums = '''.!\"#$%&()*+,-./:;<=>?@[\\\\]^_{|}~`''' + '0123456789'\n",
    "def removepunct(corpus):\n",
    "    translator = str.maketrans('', '', punct_and_nums)\n",
    "    return corpus.apply(lambda x: x.translate(translator))\n",
    "\n",
    "# Remove stopwords from sequences\n",
    "def clearstopwords(sequences):\n",
    "    filtered = []\n",
    "    stopwords_ = stopwords.words('english')\n",
    "    for sequence in sequences:\n",
    "        seq = [word for word in sequence.split() if word not in stopwords_]\n",
    "        filtered.append(\" \".join(seq))\n",
    "    return pd.Series(filtered)\n",
    "\n",
    "def lemmatizetext(corpus):\n",
    "    return corpus.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "def clearshortwords(corpus, min_len=3, max_len=17):\n",
    "    lenfilter = lambda x: \" \".join([w for w in x.split() if min_len <= len(w) <= max_len])\n",
    "    return corpus.apply(lenfilter)\n",
    "\n",
    "def clearnonascii(corpus):\n",
    "    return corpus.apply(lambda x: ''.join([char for char in x if ord(char) < 128]))\n",
    "\n",
    "def remove_urls(corpus):\n",
    "    return corpus.apply(lambda x: re.sub(r'http\\S+|www\\S+|https\\S+', '', x))\n",
    "\n",
    "def drop_short(corpus, min_length=2):\n",
    "    corpus = corpus.dropna()\n",
    "    return corpus[corpus.email.apply(lambda x: len(x.split()) > min_length)]\n",
    "\n",
    "def stats(corpus):\n",
    "    tokens = [word for sentence in corpus for word in sentence.split()]\n",
    "    vocab = {word: idx for idx, word in enumerate(set(tokens))}\n",
    "    return vocab, tokens\n",
    "\n",
    "# Split sequences into individual words (tokenization)\n",
    "def removespaces(corpus): \n",
    "    return corpus.apply(lambda x: \" \".join(x.split()))\n",
    "# Flatten a list of lists into a single list of words\n",
    "def flatten(corpus): \n",
    "    return [word for sublist in corpus for word in sublist.split()]\n",
    "\n",
    "# Main preprocessing pipeline\n",
    "def preprocess(corpus):\n",
    "    corpus = lowercase(corpus)\n",
    "    print(\"Lowercased...\")\n",
    "    corpus = removepunct(corpus)\n",
    "    print(\"Punctuations Removed...\")\n",
    "    corpus = clearshortwords(corpus)\n",
    "    print(\"Short words Removed...\")\n",
    "    corpus = clearnonascii(corpus)\n",
    "    print(\"Non ascii chars Removed...\")\n",
    "    corpus = remove_urls(corpus)\n",
    "    print(\"URLs Removed...\")\n",
    "    corpus = removespaces(corpus)\n",
    "    print(\"Extra Spaces Removed...\")\n",
    "    corpus = clearstopwords(corpus)\n",
    "    print(\"Stopwords Removed...\")\n",
    "    corpus = lemmatizetext(corpus)\n",
    "    print(\"Lemmatized Text...\")\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased...\n",
      "Punctuations Removed...\n",
      "Short words Removed...\n",
      "Non ascii chars Removed...\n",
      "URLs Removed...\n",
      "Extra Spaces Removed...\n",
      "Stopwords Removed...\n",
      "Lemmatized Text...\n",
      "412737\n",
      "26311\n",
      "3038\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>date wed number aug number number number numbe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>martin posted tasso papadopoulos greek sculpto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>man threatens explosion moscow thursday august...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klez virus die already prolific virus ever kle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adding cream spaghetti carbonara effect pasta ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               email  label\n",
       "0  date wed number aug number number number numbe...      0\n",
       "1  martin posted tasso papadopoulos greek sculpto...      0\n",
       "2  man threatens explosion moscow thursday august...      0\n",
       "3  klez virus die already prolific virus ever kle...      0\n",
       "4  adding cream spaghetti carbonara effect pasta ...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = pd.read_csv('dataset/spam_or_not_spam.csv.csv')\n",
    "file = file.drop_duplicates(subset='email', keep='first').dropna()\n",
    "sequences, labels = file['email'], file['label']\n",
    "sequences = preprocess(sequences)\n",
    "vocab,tokens = stats(sequences)\n",
    "file['email'] = sequences\n",
    "file = drop_short(file)\n",
    "print(len(tokens))\n",
    "print(len(vocab))\n",
    "print(len(file))\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.to_csv(\"dataset/email_spam_augp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class Word2Vec():\n",
    "    \"\"\"A Word2Vec implementation using the Skip-gram model with negative sampling.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_size=10, window_size=2, num_negative=5, learning_rate=0.01):\n",
    "        \"\"\"Initializes Word2Vec with hyperparameters and placeholders for embeddings and vocabulary.\"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.num_negative = num_negative\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.vocab_size = 0\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.target_embeds = None\n",
    "        self.context_embeds = None\n",
    "        self.word_freqs = None\n",
    "        \n",
    "        # Pre-compute sigmoid table for faster computation\n",
    "        self.sigmoid_table = np.zeros(1000)\n",
    "        for i in range(1000):\n",
    "            self.sigmoid_table[i] = 1/(1 + np.exp(-(i/100 - 5)))\n",
    "    \n",
    "    def fast_sigmoid(self, x):\n",
    "        \"\"\"Faster sigmoid using pre-computed table\"\"\"\n",
    "        x = 100 * x + 500  # Scale to [0,1000]\n",
    "        x = np.clip(x, 0, 999).astype(int)\n",
    "        return self.sigmoid_table[x]\n",
    "\n",
    "    def build_vocab(self, sequences):\n",
    "        \"\"\"Builds vocabulary and initializes embeddings from a list of sequences.\"\"\"\n",
    "        # Process all sequences at once for better efficiency\n",
    "        all_words = ' '.join(sequences).split()\n",
    "        word_counts = Counter(all_words)\n",
    "        \n",
    "        # Build vocabulary\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(word_counts.keys())}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "        self.vocab_size = len(word_counts)\n",
    "\n",
    "        # Calculate frequencies\n",
    "        total_words = len(all_words)\n",
    "        self.word_freqs = np.zeros(self.vocab_size)\n",
    "        for word, idx in self.word_to_idx.items():\n",
    "            self.word_freqs[idx] = word_counts[word] / total_words\n",
    "\n",
    "        # Initialize embeddings with better scaling\n",
    "        scale = 0.5/self.embedding_size  # Better initialization\n",
    "        self.target_embeds = np.random.uniform(-scale, scale, (self.vocab_size, self.embedding_size))\n",
    "        self.context_embeds = np.random.uniform(-scale, scale, (self.vocab_size, self.embedding_size))\n",
    "        \n",
    "        # Pre-compute sampling table for negative sampling\n",
    "        self.sampling_weights = np.power(self.word_freqs, 0.75)\n",
    "        self.sampling_weights /= np.sum(self.sampling_weights)\n",
    "\n",
    "    def negative_sampling(self, context_word):\n",
    "        \"\"\"Samples negative words for a given context word.\"\"\"\n",
    "        return np.random.choice(\n",
    "            self.vocab_size, \n",
    "            size=self.num_negative, \n",
    "            p=self.sampling_weights\n",
    "        )\n",
    "\n",
    "    def train_step(self, target_idx, context_idx):\n",
    "        \"\"\"Performs a single training step for a target-context pair.\"\"\"\n",
    "        # Vectorized forward pass for positive sample\n",
    "        target_embedding = self.target_embeds[target_idx]\n",
    "        context_embedding = self.context_embeds[context_idx]\n",
    "        pos_score = np.dot(target_embedding, context_embedding)\n",
    "        pos_sigmoid = self.fast_sigmoid(pos_score)\n",
    "        pos_loss = -np.log(pos_sigmoid + 1e-10)\n",
    "\n",
    "        # Gradient for positive sample\n",
    "        pos_grad = pos_sigmoid - 1\n",
    "        target_grad = pos_grad * context_embedding\n",
    "        context_grad = pos_grad * target_embedding\n",
    "\n",
    "        # Get negative samples all at once\n",
    "        neg_indices = self.negative_sampling(target_idx)\n",
    "        neg_embeddings = self.context_embeds[neg_indices]\n",
    "        \n",
    "        # Vectorized forward pass for negative samples\n",
    "        neg_scores = np.dot(target_embedding, neg_embeddings.T)\n",
    "        neg_sigmoids = self.fast_sigmoid(neg_scores)\n",
    "        neg_loss = -np.sum(np.log(1 - neg_sigmoids + 1e-10))\n",
    "\n",
    "        # Vectorized gradient updates for negative samples\n",
    "        for i, neg_idx in enumerate(neg_indices):\n",
    "            neg_grad = neg_sigmoids[i]\n",
    "            target_grad += neg_grad * self.context_embeds[neg_idx]\n",
    "            self.context_embeds[neg_idx] -= self.learning_rate * (neg_grad * target_embedding)\n",
    "\n",
    "        # Update embeddings\n",
    "        self.target_embeds[target_idx] -= self.learning_rate * target_grad\n",
    "        self.context_embeds[context_idx] -= self.learning_rate * context_grad\n",
    "\n",
    "        return pos_loss + neg_loss\n",
    "\n",
    "    def fit(self, corpus, epochs=5, batch_size=256):\n",
    "        \"\"\"Trains the Word2Vec model on the given corpus.\"\"\"\n",
    "        if not self.word_to_idx:\n",
    "            self.build_vocab(corpus)\n",
    "\n",
    "        # Pre-process corpus into indices\n",
    "        corpus_indices = [\n",
    "            [self.word_to_idx[word] for word in seq.split() if word in self.word_to_idx]\n",
    "            for seq in corpus\n",
    "        ]\n",
    "        corpus_indices = [idx for idx in corpus_indices if len(idx) >= 2]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "            \n",
    "            # Split data into batches for training\n",
    "            n_batches = max(1, len(corpus_indices) // batch_size)\n",
    "            for batch in range(n_batches):\n",
    "                batch_loss = 0\n",
    "                start_idx = batch * batch_size\n",
    "                end_idx = min((batch + 1) * batch_size, len(corpus_indices))\n",
    "                \n",
    "                # Process each sequence in the current batch\n",
    "                for sequence in corpus_indices[start_idx:end_idx]:\n",
    "                    for i, target_idx in enumerate(sequence):\n",
    "                        # Define the context window around the target word\n",
    "                        start = max(0, i-self.window_size)\n",
    "                        end = min(len(sequence), i + self.window_size + 1)\n",
    "                        context_indices = sequence[start:i] + sequence[i+1:end]\n",
    "                        \n",
    "                        # Train on all target-context pairs in the current window\n",
    "                        for context_idx in context_indices:\n",
    "                            loss = self.train_step(target_idx, context_idx)\n",
    "                            batch_loss += loss\n",
    "                \n",
    "                # Accumulate epoch loss\n",
    "                epoch_loss += batch_loss\n",
    "                if batch % max(1, n_batches//10) == 0:\n",
    "                    print(f\"Batch {batch+1}/{n_batches} | Loss: {batch_loss/batch_size:.4f}\")\n",
    "            \n",
    "            print(f\"Epoch {epoch+1} complete | Average Loss: {epoch_loss/len(corpus_indices):.4f}\")\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"Retrieves the embedding for a given word.\"\"\"\n",
    "        if word not in self.word_to_idx: return None\n",
    "        return self.target_embeds[self.word_to_idx[word]]\n",
    "    \n",
    "w2v = Word2Vec(embedding_size=10, window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Batch 1/11 | Loss: 2072.5327\n",
      "Batch 2/11 | Loss: 2502.3020\n",
      "Batch 3/11 | Loss: 3685.1877\n",
      "Batch 4/11 | Loss: 2408.3652\n",
      "Batch 5/11 | Loss: 1332.7333\n",
      "Batch 6/11 | Loss: 1282.1414\n",
      "Batch 7/11 | Loss: 1442.9730\n",
      "Batch 8/11 | Loss: 609.7445\n",
      "Batch 9/11 | Loss: 508.1130\n",
      "Batch 10/11 | Loss: 1283.5275\n",
      "Batch 11/11 | Loss: 2566.7053\n",
      "Epoch 1 complete | Average Loss: 1756.7064\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 1/11 | Loss: 1482.3738\n",
      "Batch 2/11 | Loss: 1817.2706\n",
      "Batch 3/11 | Loss: 2736.1248\n",
      "Batch 4/11 | Loss: 1846.1073\n",
      "Batch 5/11 | Loss: 1003.8028\n",
      "Batch 6/11 | Loss: 1016.0433\n",
      "Batch 7/11 | Loss: 1179.7409\n",
      "Batch 8/11 | Loss: 523.3938\n",
      "Batch 9/11 | Loss: 443.7926\n",
      "Batch 10/11 | Loss: 1095.4424\n",
      "Batch 11/11 | Loss: 2228.1720\n",
      "Epoch 2 complete | Average Loss: 1371.1846\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 1/11 | Loss: 1333.8813\n",
      "Batch 2/11 | Loss: 1636.2117\n",
      "Batch 3/11 | Loss: 2487.8980\n",
      "Batch 4/11 | Loss: 1697.7216\n",
      "Batch 5/11 | Loss: 913.8961\n",
      "Batch 6/11 | Loss: 934.6386\n",
      "Batch 7/11 | Loss: 1099.5998\n",
      "Batch 8/11 | Loss: 497.0571\n",
      "Batch 9/11 | Loss: 422.8170\n",
      "Batch 10/11 | Loss: 1035.9637\n",
      "Batch 11/11 | Loss: 2102.0356\n",
      "Epoch 3 complete | Average Loss: 1263.2057\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 1/11 | Loss: 1271.4372\n",
      "Batch 2/11 | Loss: 1553.4110\n",
      "Batch 3/11 | Loss: 2370.5747\n",
      "Batch 4/11 | Loss: 1622.8265\n",
      "Batch 5/11 | Loss: 865.2202\n",
      "Batch 6/11 | Loss: 890.5726\n",
      "Batch 7/11 | Loss: 1057.5091\n",
      "Batch 8/11 | Loss: 482.0946\n",
      "Batch 9/11 | Loss: 411.2769\n",
      "Batch 10/11 | Loss: 1005.1760\n",
      "Batch 11/11 | Loss: 2028.9985\n",
      "Epoch 4 complete | Average Loss: 1209.4526\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 1/11 | Loss: 1233.4560\n",
      "Batch 2/11 | Loss: 1502.1450\n",
      "Batch 3/11 | Loss: 2297.0012\n",
      "Batch 4/11 | Loss: 1573.3004\n",
      "Batch 5/11 | Loss: 831.3438\n",
      "Batch 6/11 | Loss: 862.7105\n",
      "Batch 7/11 | Loss: 1029.7379\n",
      "Batch 8/11 | Loss: 473.1059\n",
      "Batch 9/11 | Loss: 404.2607\n",
      "Batch 10/11 | Loss: 983.0896\n",
      "Batch 11/11 | Loss: 1976.4039\n",
      "Epoch 5 complete | Average Loss: 1174.4383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3888842.880670882"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.fit(sequences, epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Save Word: Embedding pairs into file \n",
    "def save_embeddings(model, file_path='embeddings.npy'):\n",
    "    embeddings = {word: model.target_embeds[idx] for word, idx in model.word_to_idx.items()}\n",
    "    np.save(file_path, embeddings)\n",
    "    print(f\"Embeddings saved to {file_path}\")\n",
    "\n",
    "# Function to Load Word Embeddings\n",
    "def load_embeddings(file_path='embeddings.npy'):\n",
    "    return np.load(file_path, allow_pickle=True).item()\n",
    "\n",
    "save_embeddings(w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "# Load Processed Corpus\n",
    "file = pd.read_csv('/kaggle/input/email-sns/email_spam.csv')\n",
    "sequences, labels = file.email.values, file.label.values \n",
    "\n",
    "# Separate data by classes\n",
    "minority_class_texts = [sequences[i] for i in range(len(labels)) if labels[i] == 1]\n",
    "majority_class_texts = [sequences[i] for i in range(len(labels)) if labels[i] == 0]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(len(majority_class_texts), len(minority_class_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load translation models for back-translation\n",
    "src_model_name = \"Helsinki-NLP/opus-mt-en-fr\"  # English to French\n",
    "tgt_model_name = \"Helsinki-NLP/opus-mt-fr-en\"  # French to English\n",
    "src_tokenizer = MarianTokenizer.from_pretrained(src_model_name)\n",
    "src_model = MarianMTModel.from_pretrained(src_model_name).to(device)\n",
    "tgt_tokenizer = MarianTokenizer.from_pretrained(tgt_model_name)\n",
    "tgt_model = MarianMTModel.from_pretrained(tgt_model_name).to(device)\n",
    "\n",
    "def back_translate(text):\n",
    "    # Translate from English to French\n",
    "    inputs = src_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad(): translated = src_model.generate(**inputs)\n",
    "    french_text = src_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "    # Translate back from French to English\n",
    "    inputs = tgt_tokenizer(french_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad(): back_translated = tgt_model.generate(**inputs)\n",
    "    return tgt_tokenizer.decode(back_translated[0], skip_special_tokens=True)\n",
    "    \n",
    "augmented_texts = []\n",
    "for i, text in enumerate(minority_class_texts):\n",
    "    try:\n",
    "        # Apply back-translation\n",
    "        back_translated = back_translate(text)\n",
    "        augmented_texts.append(back_translated)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}, {e}\")\n",
    "    if i % (len(minority_class_texts)//10) == 0: print(f\"{i}/{len(minority_class_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for augmented data\n",
    "augmented_labels = [1] * len(augmented_texts)\n",
    "\n",
    "# Combine original and augmented data\n",
    "all_texts = sequences.tolist() + augmented_texts\n",
    "all_labels = labels.tolist() + augmented_labels\n",
    "pos = np.count_nonzero(all_labels)\n",
    "print(pos, len(all_labels) - pos)\n",
    "\n",
    "augmented_df = pd.DataFrame({\"email\": all_texts, \"label\": all_labels})\n",
    "augmented_df.to_csv(\"email_spam_aug.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
