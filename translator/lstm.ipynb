{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-16T17:32:26.417432Z",
     "iopub.status.busy": "2024-11-16T17:32:26.416630Z",
     "iopub.status.idle": "2024-11-16T17:32:26.424261Z",
     "shell.execute_reply": "2024-11-16T17:32:26.422874Z",
     "shell.execute_reply.started": "2024-11-16T17:32:26.417381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T17:32:26.913911Z",
     "iopub.status.busy": "2024-11-16T17:32:26.913405Z",
     "iopub.status.idle": "2024-11-16T17:32:27.132087Z",
     "shell.execute_reply": "2024-11-16T17:32:27.130759Z",
     "shell.execute_reply.started": "2024-11-16T17:32:26.913864Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Size: 13400\n",
      "Test Dataset Size: 457\n",
      "Dev Dataset Size: 514\n"
     ]
    }
   ],
   "source": [
    "data_path = '/kaggle/input/umc005'\n",
    "\n",
    "# Function to load a text corpus from a specified file path\n",
    "def load_corpus(file_path, set_path):\n",
    "    set_path = os.path.join(data_path, set_path)\n",
    "    file_path = os.path.join(set_path, file_path)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "# Load English and Urdu training, validation, and test data from Quran and Bible datasets\n",
    "train_en = load_corpus('train.en', 'quran') + load_corpus('train.en', 'bible') \n",
    "train_ur = load_corpus('train.ur', 'quran') + load_corpus('train.ur', 'bible')\n",
    "dev_en = load_corpus('dev.en', 'quran') + load_corpus('dev.en', 'bible')\n",
    "dev_ur = load_corpus('dev.ur', 'quran') + load_corpus('dev.ur', 'bible')\n",
    "test_en = load_corpus('test.en', 'quran') + load_corpus('test.en', 'bible')\n",
    "test_ur = load_corpus('test.ur', 'quran') + load_corpus('test.ur', 'bible')\n",
    "en_corpus = train_en + test_en + dev_en\n",
    "ur_corpus = train_ur + test_ur + dev_ur\n",
    "\n",
    "# Check that the dataset pairs are properly aligned\n",
    "assert len(train_en) == len(train_ur), \"Training data misaligned!\"\n",
    "assert len(dev_en) == len(dev_ur), \"Validation data misaligned!\"\n",
    "assert len(test_en) == len(test_ur), \"Test data misaligned!\"\n",
    "print(\"Train Dataset Size:\", len(train_en))\n",
    "print(\"Test Dataset Size:\", len(test_en))\n",
    "print(\"Dev Dataset Size:\", len(dev_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T17:33:48.524592Z",
     "iopub.status.busy": "2024-11-16T17:33:48.524145Z",
     "iopub.status.idle": "2024-11-16T17:33:49.171249Z",
     "shell.execute_reply": "2024-11-16T17:33:49.169917Z",
     "shell.execute_reply.started": "2024-11-16T17:33:48.524554Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Saved to file vocab_en\n",
      "Vocabulary Saved to file vocab_ur\n",
      "\n",
      "English Vocabulary Size: 11115\n",
      "Urdu Vocabulary Size: 11010\n"
     ]
    }
   ],
   "source": [
    "# Maximum vocabulary sizes and sequence lengths for English and Urdu\n",
    "max_input_length = 150\n",
    "max_output_length = 181\n",
    "\n",
    "src_vocab_size = 11115\n",
    "tgt_vocab_size = 11015\n",
    "\n",
    "embedding_dim = 256\n",
    "hidden_units = 512\n",
    "\n",
    "# Cleaning Urdu text: Remove non-alphanumeric characters and add START/END tokens\n",
    "def clean_urdu(text):\n",
    "    # Remove non-alphanumeric and special characters, keeping Urdu script\n",
    "    text = tf.strings.regex_replace(text, r'[^\\p{L}\\s]', '')  # Keep letters and spaces\n",
    "    text = tf.strings.regex_replace(text, r'\\s+', ' ')  # Normalize whitespace\n",
    "    text = tf.strings.strip(text)  # Remove leading/trailing spaces\n",
    "    text = tf.strings.join(['START', text, 'END'], separator=\" \")\n",
    "    return text\n",
    "\n",
    "# Cleaning English text: Lowercase and remove non-alphabetic characters\n",
    "def clean_english(text):\n",
    "    text = tf.strings.lower(text)  # Convert to lowercase\n",
    "    text = tf.strings.regex_replace(text, r'[^a-zA-Z\\s]', '')  # Remove non-alphabetic characters\n",
    "    text = tf.strings.regex_replace(text, r'\\s+', ' ')  # Normalize whitespace\n",
    "    text = tf.strings.strip(text)  # Remove leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "# Corpus cleaning function based on language\n",
    "def clean_corpus(corpus, lang='en'):\n",
    "    if lang == 'en': return [clean_english(t) for t in corpus]\n",
    "    elif lang == 'ur': return [clean_urdu(t) for t in corpus]\n",
    "\n",
    "# Save tokenizer vocabulary to a file\n",
    "def save_tokenizer(vectorizer, filename, verbose=None):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(\"\\n\".join(vectorizer.get_vocabulary()))\n",
    "    file.close()\n",
    "    if verbose: print(f\"Vocabulary Saved to file {filename}\")\n",
    "\n",
    "# Load or create a tokenizer using TextVectorization\n",
    "def load_tokenizer(lang_ds, max_vocab, seq_len, standardize, pretrained=None):\n",
    "    vectorizer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=max_vocab,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=seq_len,\n",
    "        standardize=standardize,\n",
    "    )\n",
    "    vectorizer.adapt(lang_ds)        \n",
    "    return vectorizer\n",
    "    \n",
    "# Initialize and save English and Urdu tokenizers\n",
    "tokenizer_en = load_tokenizer(en_corpus, src_vocab_size, max_input_length, clean_english)\n",
    "tokenizer_ur = load_tokenizer(ur_corpus, tgt_vocab_size, max_output_length, clean_urdu)\n",
    "save_tokenizer(tokenizer_en, 'vocab_en', True)\n",
    "save_tokenizer(tokenizer_ur, 'vocab_ur', True)\n",
    "\n",
    "# Print vocabulary sizes for English and Urdu\n",
    "en_vocab_size = tokenizer_en.vocabulary_size()\n",
    "ur_vocab_size = tokenizer_ur.vocabulary_size()\n",
    "print(f\"\\nEnglish Vocabulary Size: {en_vocab_size}\")\n",
    "print(f\"Urdu Vocabulary Size: {ur_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T17:33:49.174348Z",
     "iopub.status.busy": "2024-11-16T17:33:49.173804Z",
     "iopub.status.idle": "2024-11-16T17:33:50.015261Z",
     "shell.execute_reply": "2024-11-16T17:33:50.013577Z",
     "shell.execute_reply.started": "2024-11-16T17:33:49.174291Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Batch: tf.Tensor(\n",
      "[  11   27 1439 1337    3  476   37   11   22   15  413    5    2  277\n",
      "  262    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0], shape=(150,), dtype=int64)\n",
      "\n",
      "Urdu Input Batch (Decoder Input): tf.Tensor(\n",
      "[   3   27 1896 2618    2 1037   16   63   13  144 1259   10   57   24\n",
      " 6107   33    4    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(180,), dtype=int64)\n",
      "\n",
      "Urdu Target Batch (Decoder Target): tf.Tensor(\n",
      "[  27 1896 2618    2 1037   16   63   13  144 1259   10   57   24 6107\n",
      "   33    4    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(180,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Create dataset for training and validation\n",
    "def create_dataset(en_ds, ur_ds, batch_size):\n",
    "    # Convert lists to TensorFlow datasets\n",
    "    en_dataset = tf.data.Dataset.from_tensor_slices(en_ds)\n",
    "    ur_dataset = tf.data.Dataset.from_tensor_slices(ur_ds)\n",
    "\n",
    "    # Tokenize English and Urdu datasets\n",
    "    en_dataset = en_dataset.map(tokenizer_en)\n",
    "    ur_dataset = ur_dataset.map(tokenizer_ur)\n",
    "\n",
    "    # Prepare decoder input and target from Urdu tokenized data\n",
    "    ur_input_dataset = ur_dataset.map(lambda x: x[:-1])\n",
    "    ur_target_dataset = ur_dataset.map(lambda x: x[1:])\n",
    "    # Zip English and Urdu targets, Batch the dataset and enable prefetching\n",
    "    dataset = tf.data.Dataset.zip(((en_dataset, ur_input_dataset), ur_target_dataset))\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "batch_size = 64\n",
    "train_ds = create_dataset(train_en, train_ur, batch_size)\n",
    "val_ds = create_dataset(dev_en, dev_ur, batch_size)\n",
    "\n",
    "# Randomly sample a batch and display tokenized examples\n",
    "idx = np.random.randint(batch_size)\n",
    "for (en, ur), ur_labels in train_ds.take(1):\n",
    "    print(\"English Batch:\", en[idx])\n",
    "    print(\"\\nUrdu Input Batch (Decoder Input):\", ur[idx])\n",
    "    print(\"\\nUrdu Target Batch (Decoder Target):\", ur_labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T17:35:23.279620Z",
     "iopub.status.busy": "2024-11-16T17:35:23.279020Z",
     "iopub.status.idle": "2024-11-16T17:35:23.846768Z",
     "shell.execute_reply": "2024-11-16T17:35:23.845090Z",
     "shell.execute_reply.started": "2024-11-16T17:35:23.279567Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_4         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,845,440</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_5         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,845,440</span> │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ embedding_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ embedding_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>,       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,648,130</span> │ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">11010</span>)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_4         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m2,845,440\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_5         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m2,845,440\u001b[0m │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),     │  \u001b[38;5;34m1,574,912\u001b[0m │ embedding_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m,      │  \u001b[38;5;34m1,574,912\u001b[0m │ embedding_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n",
       "│                     │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n",
       "│                     │ \u001b[38;5;34m512\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m,       │  \u001b[38;5;34m5,648,130\u001b[0m │ lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│                     │ \u001b[38;5;34m11010\u001b[0m)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,488,834</span> (55.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,488,834\u001b[0m (55.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,488,834</span> (55.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,488,834\u001b[0m (55.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def LSTM_model(pretrained_weights=None):\n",
    "    \"\"\"\n",
    "    Builds a sequence-to-sequence LSTM model for machine translation.\n",
    "    The model includes an encoder-decoder architecture with LSTM cells and embeddings.\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    # Encoder part\n",
    "    encoder_input = tf.keras.layers.Input(shape=(max_input_length,))  # Input shape for the encoder\n",
    "    embedding = tf.keras.layers.Embedding(src_vocab_size, embedding_dim)(encoder_input)  # Embedding layer for encoder\n",
    "    encoder_lstm, state_h, state_c = tf.keras.layers.LSTM(hidden_units, return_state=True)(embedding)\n",
    "    encoder_states = [state_h, state_c]  # The encoder state is passed to the decoder\n",
    "\n",
    "    # Decoder part\n",
    "    decoder_input = tf.keras.layers.Input(shape=(max_output_length-1,))  # Input shape for the decoder (one less due to teacher forcing)\n",
    "    decoder_embedding = tf.keras.layers.Embedding(en_vocab_size, embedding_dim)(decoder_input)  # Embedding layer for decoder\n",
    "    decoder_lstm = tf.keras.layers.LSTM(hidden_units, return_sequences=True, return_state=True)  # Decoder LSTM\n",
    "    decoder_output, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)  # Decoder with initial state from encoder\n",
    "    decoder_dense = tf.keras.layers.Dense(ur_vocab_size, activation='softmax')  # Output layer for predicting the next token\n",
    "    decoder_output = decoder_dense(decoder_output)  # The decoder output is passed through a dense layer to predict token probabilities\n",
    "\n",
    "    # Define and Build the model with the specified input shape\n",
    "    model = tf.keras.models.Model([encoder_input, decoder_input], decoder_output)\n",
    "    model.build(input_shape=(None, max_input_length))\n",
    "    \n",
    "    # If pre-trained weights are provided, load them into the model\n",
    "    if pretrained_weights: \n",
    "        model.load_weights(pretrained_weights)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "batch_size = 32  # Define the batch size for training\n",
    "# Create and compile the model\n",
    "model = LSTM_model()  # Instantiate the model\n",
    "model.compile(loss='sparse_categorical_crossentropy',  # Loss function (for multi-class classification)\n",
    "              optimizer=Adam(learning_rate=1e-3),  # Adam optimizer with learning rate 1e-3\n",
    "              metrics=['accuracy'])  # Track accuracy during training\n",
    "model.summary()  # Print model summary to check the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T17:35:25.382060Z",
     "iopub.status.busy": "2024-11-16T17:35:25.381548Z",
     "iopub.status.idle": "2024-11-16T17:35:25.388691Z",
     "shell.execute_reply": "2024-11-16T17:35:25.387204Z",
     "shell.execute_reply.started": "2024-11-16T17:35:25.382015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(filepath='best_modelmt.keras', save_best_only=True, monitor='val_loss', mode='min'),\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    CSVLogger('losses.csv')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T17:35:35.736858Z",
     "iopub.status.busy": "2024-11-16T17:35:35.736238Z",
     "iopub.status.idle": "2024-11-16T20:26:08.835024Z",
     "shell.execute_reply": "2024-11-16T20:26:08.833072Z",
     "shell.execute_reply.started": "2024-11-16T17:35:35.736777Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2039s\u001b[0m 10s/step - accuracy: 0.7676 - loss: 2.3915 - val_accuracy: 0.8731 - val_loss: 0.8191\n",
      "Epoch 2/5\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2045s\u001b[0m 10s/step - accuracy: 0.8097 - loss: 1.1821 - val_accuracy: 0.8804 - val_loss: 0.7421\n",
      "Epoch 4/5\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2039s\u001b[0m 10s/step - accuracy: 0.8145 - loss: 1.1191 - val_accuracy: 0.8825 - val_loss: 0.7264\n",
      "Epoch 5/5\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2055s\u001b[0m 10s/step - accuracy: 0.8185 - loss: 1.0761 - val_accuracy: 0.8841 - val_loss: 0.7145\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T20:26:09.802328Z",
     "iopub.status.busy": "2024-11-16T20:26:09.801770Z",
     "iopub.status.idle": "2024-11-16T20:26:10.512912Z",
     "shell.execute_reply": "2024-11-16T20:26:10.511591Z",
     "shell.execute_reply.started": "2024-11-16T20:26:09.802280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save('lstm_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T20:32:34.486604Z",
     "iopub.status.busy": "2024-11-16T20:32:34.486167Z",
     "iopub.status.idle": "2024-11-16T20:32:34.502090Z",
     "shell.execute_reply": "2024-11-16T20:32:34.500641Z",
     "shell.execute_reply.started": "2024-11-16T20:32:34.486552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def translate_lstm_model(sentence, model, tokenizer_en, tokenizer_ur, max_input_length=150, max_output_length=5):\n",
    "    # Step 1: Clean and tokenize the input sentence\n",
    "    input_tensor = tf.constant([sentence])  # Wrap the sentence as a batch of size 1\n",
    "    input_indices = tokenizer_en(input_tensor).numpy()[0]  # Convert to token indices\n",
    "    input_indices = np.pad(input_indices, (0, max_input_length - len(input_indices)), 'constant')  # Pad to max length\n",
    "\n",
    "    # Step 2: Prepare the encoder input\n",
    "    encoder_input = tf.constant([input_indices])  # Batch of size 1, shape (1, max_input_length)\n",
    "    \n",
    "    # Step 3: Get the initial encoder states\n",
    "    encoder_embedding = model.get_layer('embedding_4')(encoder_input)\n",
    "    encoder_lstm = model.get_layer('lstm_4')  # Make sure to use the correct layer name\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Step 4: Initialize decoder input with the START token\n",
    "    start_token = tokenizer_ur([\"START\"]).numpy()[0][0]  # Convert \"START\" token to index\n",
    "    decoder_input = np.array([start_token])\n",
    "    \n",
    "    # Step 5: Initialize the decoded sentence list\n",
    "    translated_tokens = []\n",
    "\n",
    "    # Step 6: Decode word by word until the \"END\" token is predicted or max length is reached\n",
    "    for _ in range(max_output_length):\n",
    "        # Step 6.1: Predict the next token with the current decoder input and encoder states\n",
    "        decoder_input_tensor = tf.constant([decoder_input])  # Convert to tensor of shape (1, 1)\n",
    "        \n",
    "        decoder_embedding = model.get_layer('embedding_5')(decoder_input_tensor)  # Decoder embedding layer\n",
    "        decoder_lstm = model.get_layer('lstm_5')  # Decoder LSTM\n",
    "        decoder_output, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "        \n",
    "        # Step 6.2: Get the predicted word (index of max probability)\n",
    "        predicted_index = np.argmax(decoder_output[0, -1, :].numpy())  # Get index of most probable word\n",
    "        \n",
    "        if predicted_index == tokenizer_ur([\"END\"]).numpy()[0][0]:\n",
    "            break  # Stop if END token is predicted\n",
    "        \n",
    "        translated_tokens.append(predicted_index)  # Append predicted token to the output\n",
    "        \n",
    "        # Step 6.3: Update decoder input to the predicted token for the next step\n",
    "        decoder_input = np.array([predicted_index])\n",
    "\n",
    "    # Step 7: Convert the token indices back to words\n",
    "    translated_sentence = ' '.join([tokenizer_ur.get_vocabulary()[idx] for idx in translated_tokens])\n",
    "    \n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T20:33:06.818642Z",
     "iopub.status.busy": "2024-11-16T20:33:06.817251Z",
     "iopub.status.idle": "2024-11-16T20:33:07.984944Z",
     "shell.execute_reply": "2024-11-16T20:33:07.983007Z",
     "shell.execute_reply.started": "2024-11-16T20:33:06.818586Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Sentence: جائے ہاں جائے ہاں جائے\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Ho how are you ?\"  # Example input in English\n",
    "translated_sentence = translate_lstm_model(\n",
    "    sentence, model, tokenizer_en, tokenizer_ur\n",
    ")\n",
    "print(\"Translated Sentence:\", translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6080890,
     "sourceId": 9925481,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
