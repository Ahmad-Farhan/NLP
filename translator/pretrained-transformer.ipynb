{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9899371,"sourceType":"datasetVersion","datasetId":6080890}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-11-16T13:14:07.045695Z","iopub.execute_input":"2024-11-16T13:14:07.046468Z","iopub.status.idle":"2024-11-16T13:14:07.056308Z","shell.execute_reply.started":"2024-11-16T13:14:07.046429Z","shell.execute_reply":"2024-11-16T13:14:07.055397Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data_path = '/kaggle/input/umc005/bible'\n\ndef load_corpus(file_path):\n    file_path = os.path.join(data_path, file_path)\n    with open(file_path, 'r', encoding='utf-8') as file:\n        return file.readlines()\n\ntrain_en = load_corpus('train.en')\ntrain_ur = load_corpus('train.ur')\ndev_en = load_corpus('dev.en')\ndev_ur = load_corpus('dev.ur')\ntest_en = load_corpus('test.en')\ntest_ur = load_corpus('test.ur')\nen_corpus = train_en + test_en + dev_en\nur_corpus = train_ur + test_ur + dev_ur\n\nassert len(train_en) == len(train_ur), \"Training data misaligned!\"\nassert len(dev_en) == len(dev_ur), \"Validation data misaligned!\"\nassert len(test_en) == len(test_ur), \"Test data misaligned!\"\nprint(\"Train Dataset Size:\", len(train_en))\nprint(\"Test Dataset Size:\", len(test_en))\nprint(\"Dev Dataset Size:\", len(dev_en))","metadata":{"execution":{"iopub.status.busy":"2024-11-16T13:14:07.058316Z","iopub.execute_input":"2024-11-16T13:14:07.058723Z","iopub.status.idle":"2024-11-16T13:14:07.167387Z","shell.execute_reply.started":"2024-11-16T13:14:07.058680Z","shell.execute_reply":"2024-11-16T13:14:07.166449Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Train Dataset Size: 7400\nTest Dataset Size: 257\nDev Dataset Size: 300\n","output_type":"stream"}]},{"cell_type":"code","source":"max_en_vocab = 6000\nmax_ur_vocab = 7100\nen_seq_len = 65 # 70\nur_seq_len = 79 # 84 \nimport re\n\ndef clean_urdu(text):\n    # Remove non-Urdu characters (keeping only Urdu script characters and spaces)\n    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)  # Match Urdu characters and spaces\n    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n    text = text.strip()  # Remove leading/trailing spaces\n    text = 'START ' + text + ' END'  # Add start/end tokens\n    return text\n\ndef clean_english(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n    text = text.strip()  # Remove leading/trailing spaces\n    return text\n\ndef clean_corpus(corpus, lang='en'):\n    if lang == 'en': return [clean_english(t) for t in corpus]\n    elif lang == 'ur': return [clean_urdu(t) for t in corpus]\n\ntrain_en = clean_corpus(train_en,'en')\ntrain_ur = clean_corpus(train_ur,'ur')\nval_en = clean_corpus(dev_en,'en')\nval_ur = clean_corpus(dev_ur,'ur')","metadata":{"execution":{"iopub.status.busy":"2024-11-16T13:14:07.168449Z","iopub.execute_input":"2024-11-16T13:14:07.168752Z","iopub.status.idle":"2024-11-16T13:14:07.454039Z","shell.execute_reply.started":"2024-11-16T13:14:07.168720Z","shell.execute_reply":"2024-11-16T13:14:07.453053Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom datasets import Dataset\nimport numpy as np\nimport torch\n\ndef create_dataset(en_texts, ur_texts):\n    return Dataset.from_dict({\n        \"translation\": [\n            {\"en\": en, \"ur\": ur} \n            for en, ur in zip(en_texts, ur_texts)\n        ]\n    })\n\n# Create train and validation datasets\ntrain_dataset = create_dataset(train_en, train_ur)\nval_dataset = create_dataset(val_en, val_ur)\n\n# Load tokenizer and model\nmodel_name = \"Helsinki-NLP/opus-mt-en-ur\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# Preprocessing function\ndef preprocess_function(examples):\n    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n    targets = [ex[\"ur\"] for ex in examples[\"translation\"]]\n    \n    model_inputs = tokenizer(\n        inputs, \n        max_length=128, \n        truncation=True, \n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n    \n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets,\n            max_length=128,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Tokenize datasets\ntokenized_train = train_dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=train_dataset.column_names\n)\ntokenized_val = val_dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=val_dataset.column_names\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T13:14:07.455731Z","iopub.execute_input":"2024-11-16T13:14:07.456055Z","iopub.status.idle":"2024-11-16T13:14:56.585581Z","shell.execute_reply.started":"2024-11-16T13:14:07.456024Z","shell.execute_reply":"2024-11-16T13:14:56.584705Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f5d84f015274c34b176f0af29266612"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e41ce12f7628444d9e76b77c65850fbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/816k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd2a838140764da6bd5d89dfa6a8f7d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/848k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c8e464a25c24745a578b174aca17574"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2e70e0ecd47419b8ee4dec2367814bb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/306M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91b4cbc32dc5492baf79e19a47a8fa8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a781c59649b4eaea020474c396d8b67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef68b50d8d9d4a25b055c992011a7ec2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4117: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abfdd79a51c140da89160b634cd5b75c"}},"metadata":{}}]},{"cell_type":"code","source":"# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir='./results',              # output directory\n    num_train_epochs=3,                  # number of training epochs\n    per_device_train_batch_size=8,       # batch size for training\n    per_device_eval_batch_size=8,        # batch size for evaluation\n    warmup_steps=500,                    # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,                   # weight decay to avoid overfitting\n    logging_dir='./logs',                # directory for storing logs\n    logging_steps=10,                    # log every 10 steps\n    evaluation_strategy=\"epoch\",         # evaluation strategy to use\n    save_strategy=\"epoch\",               # save checkpoint every epoch\n    load_best_model_at_end=True,\n    push_to_hub=False,\n)\n\n# Create trainer\ntrainer = Seq2SeqTrainer(\n    model=model,                         # the model to train\n    args=training_args,                  # training arguments\n    train_dataset=tokenized_train,       # training dataset\n    eval_dataset=tokenized_val,          # evaluation dataset\n)\n\n# Fine-tune the model\ntrainer.train()\n\nfrom transformers import Trainer, TrainingArguments\n\n# Set training arguments\ntraining_args = TrainingArguments(\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    train_dataset=train_data,            # training dataset\n    eval_dataset=val_data,               # evaluation dataset\n)\n\n# Start fine-tuning\ntrainer.train()\nmodel.save_pretrained(\"./en-ur-finetuned\")\ntokenizer.save_pretrained(\"./en-ur-finetuned\")","metadata":{"execution":{"iopub.status.busy":"2024-11-16T13:14:56.586688Z","iopub.execute_input":"2024-11-16T13:14:56.586994Z","iopub.status.idle":"2024-11-16T13:21:17.225516Z","shell.execute_reply.started":"2024-11-16T13:14:56.586962Z","shell.execute_reply":"2024-11-16T13:21:17.224597Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113761977777711, max=1.0‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ced9f6e5f5448e9871932441377798a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241116_131533-vr8goth0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/afkun20-na/huggingface/runs/vr8goth0' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/afkun20-na/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/afkun20-na/huggingface' target=\"_blank\">https://wandb.ai/afkun20-na/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/afkun20-na/huggingface/runs/vr8goth0' target=\"_blank\">https://wandb.ai/afkun20-na/huggingface/runs/vr8goth0</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2775' max='2775' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2775/2775 05:39, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.469300</td>\n      <td>0.559828</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.426200</td>\n      <td>0.542593</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.326700</td>\n      <td>0.545793</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62024]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2775, training_loss=0.5220938926129728, metrics={'train_runtime': 378.0473, 'train_samples_per_second': 58.723, 'train_steps_per_second': 7.34, 'total_flos': 752543701401600.0, 'train_loss': 0.5220938926129728, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Load saved model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"./en-ur-finetuned\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"./en-ur-finetuned\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available(): model = model.cuda()\n\n\n# Function to translate new text\ndef translate(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return text.replace(\"START \", \"\").replace(\" END\", \"\")\n\n# Example usage\ntest_text = \"What are you doing today?\"\ntranslation = translate(test_text)\nprint(f\"English: {test_text}\")\nprint(f\"Urdu: {translation}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-16T13:29:52.976004Z","iopub.execute_input":"2024-11-16T13:29:52.976846Z","iopub.status.idle":"2024-11-16T13:29:55.180705Z","shell.execute_reply.started":"2024-11-16T13:29:52.976794Z","shell.execute_reply":"2024-11-16T13:29:55.179724Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"English: What are you doing today?\nUrdu: ÿ¢ÿ¨ ÿ™ŸÖ ⁄©€åÿß ⁄©ÿ± ÿ±€Å€í €ÅŸà ÿü €î\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}